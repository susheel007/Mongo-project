
==> Audit <==
|--------------|-------------------------------|----------|-----------------|---------|---------------------|---------------------|
|   Command    |             Args              | Profile  |      User       | Version |     Start Time      |      End Time       |
|--------------|-------------------------------|----------|-----------------|---------|---------------------|---------------------|
| dashboard    |                               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 11:18 CST |                     |
| start        |                               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 11:18 CST |                     |
| start        |                               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 12:10 CST |                     |
| delete       |                               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 12:27 CST | 19 Jan 25 12:27 CST |
| start        | --driver=docker               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 12:27 CST |                     |
| start        | --driver=docker               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 12:28 CST |                     |
| start        | --driver=docker               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 12:30 CST | 19 Jan 25 12:31 CST |
| update-check |                               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 15:41 CST | 19 Jan 25 15:41 CST |
| update-check |                               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 15:41 CST | 19 Jan 25 15:41 CST |
| update-check |                               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 15:49 CST | 19 Jan 25 15:49 CST |
| update-check |                               | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 16:13 CST | 19 Jan 25 16:13 CST |
| service      | mongo-express-service         | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 17:00 CST |                     |
| service      | mongo-express-service         | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 17:01 CST |                     |
| service      | service/mongo-express-service | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 17:02 CST |                     |
| service      | mongo-express-service         | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 17:02 CST |                     |
| service      | mongo-express-service         | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 17:07 CST |                     |
| service      | mongo-express-service         | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 17:10 CST |                     |
| service      | mongo-express                 | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 17:10 CST | 19 Jan 25 17:11 CST |
| service      | mongo-express-service         | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 17:12 CST |                     |
| service      | mongo-express-service         | minikube | susheelchennoju | v1.35.0 | 19 Jan 25 21:09 CST |                     |
|--------------|-------------------------------|----------|-----------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/01/19 12:30:41
Running on machine: susheels-mbp
Binary: Built with gc go1.23.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0119 12:30:41.925775   13136 out.go:345] Setting OutFile to fd 1 ...
I0119 12:30:41.926392   13136 out.go:397] isatty.IsTerminal(1) = true
I0119 12:30:41.926394   13136 out.go:358] Setting ErrFile to fd 2...
I0119 12:30:41.926397   13136 out.go:397] isatty.IsTerminal(2) = true
I0119 12:30:41.926638   13136 root.go:338] Updating PATH: /Users/susheelchennoju/.minikube/bin
I0119 12:30:41.927837   13136 out.go:352] Setting JSON to false
I0119 12:30:41.954433   13136 start.go:129] hostinfo: {"hostname":"susheels-mbp.lan","uptime":39851,"bootTime":1737271590,"procs":525,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.2","kernelVersion":"24.2.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"a4eb9b43-c104-5f38-843f-3411c8ce8d38"}
W0119 12:30:41.954514   13136 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0119 12:30:41.960135   13136 out.go:177] 😄  minikube v1.35.0 on Darwin 15.2 (arm64)
I0119 12:30:41.970887   13136 notify.go:220] Checking for updates...
I0119 12:30:41.971191   13136 driver.go:394] Setting default libvirt URI to qemu:///system
I0119 12:30:42.063590   13136 docker.go:123] docker version: linux-20.10.11:Docker Engine - Community
I0119 12:30:42.064088   13136 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0119 12:30:43.964192   13136 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.900090458s)
I0119 12:30:43.964907   13136 info.go:266] docker info: {ID:RWC7:EOJB:IUVI:J2DD:IBYZ:GQOI:JIKP:GUVI:7Y6I:6J5E:NFNB:ZM3R Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:false NGoroutines:53 SystemTime:2025-01-19 18:30:43.938587801 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.76-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:2085294080 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.11 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.2.1] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.14.0]] Warnings:<nil>}}
I0119 12:30:43.970713   13136 out.go:177] ✨  Using the docker driver based on user configuration
I0119 12:30:43.978848   13136 start.go:297] selected driver: docker
I0119 12:30:43.978988   13136 start.go:901] validating driver "docker" against <nil>
I0119 12:30:43.979000   13136 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0119 12:30:43.979208   13136 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0119 12:30:44.039894   13136 info.go:266] docker info: {ID:RWC7:EOJB:IUVI:J2DD:IBYZ:GQOI:JIKP:GUVI:7Y6I:6J5E:NFNB:ZM3R Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:false NGoroutines:53 SystemTime:2025-01-19 18:30:44.028653551 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.76-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:2085294080 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.11 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.2.1] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.14.0]] Warnings:<nil>}}
I0119 12:30:44.040842   13136 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0119 12:30:44.041196   13136 start_flags.go:393] Using suggested 1988MB memory alloc based on sys=8192MB, container=1988MB
I0119 12:30:44.041578   13136 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0119 12:30:44.045586   13136 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0119 12:30:44.049692   13136 cni.go:84] Creating CNI manager for ""
I0119 12:30:44.049996   13136 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0119 12:30:44.050008   13136 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0119 12:30:44.050182   13136 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:1988 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0119 12:30:44.054674   13136 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0119 12:30:44.062827   13136 cache.go:121] Beginning downloading kic base image for docker with docker
I0119 12:30:44.066598   13136 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0119 12:30:44.074971   13136 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0119 12:30:44.074972   13136 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0119 12:30:44.074990   13136 preload.go:146] Found local preload: /Users/susheelchennoju/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4
I0119 12:30:44.074994   13136 cache.go:56] Caching tarball of preloaded images
I0119 12:30:44.075119   13136 preload.go:172] Found /Users/susheelchennoju/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0119 12:30:44.075128   13136 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0119 12:30:44.075945   13136 profile.go:143] Saving config to /Users/susheelchennoju/.minikube/profiles/minikube/config.json ...
I0119 12:30:44.075995   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/profiles/minikube/config.json: {Name:mkb4fa723af3b1e073b310fd41b33ad9abe7c1e4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:30:44.091821   13136 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0119 12:30:44.092373   13136 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0119 12:30:44.092610   13136 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0119 12:30:52.699250   13136 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0119 12:30:52.699286   13136 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0119 12:31:16.256098   13136 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0119 12:31:16.257138   13136 cache.go:227] Successfully downloaded all kic artifacts
I0119 12:31:16.261417   13136 start.go:360] acquireMachinesLock for minikube: {Name:mk8e0381331ff2e760210a4287eba527c6772cc1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0119 12:31:16.265692   13136 start.go:364] duration metric: took 2.589375ms to acquireMachinesLock for "minikube"
I0119 12:31:16.266508   13136 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:1988 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0119 12:31:16.269006   13136 start.go:125] createHost starting for "" (driver="docker")
I0119 12:31:16.283860   13136 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=1988MB) ...
I0119 12:31:16.289235   13136 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0119 12:31:16.289826   13136 client.go:168] LocalClient.Create starting
I0119 12:31:16.291085   13136 main.go:141] libmachine: Reading certificate data from /Users/susheelchennoju/.minikube/certs/ca.pem
I0119 12:31:16.291708   13136 main.go:141] libmachine: Decoding PEM data...
I0119 12:31:16.291951   13136 main.go:141] libmachine: Parsing certificate...
I0119 12:31:16.295027   13136 main.go:141] libmachine: Reading certificate data from /Users/susheelchennoju/.minikube/certs/cert.pem
I0119 12:31:16.295193   13136 main.go:141] libmachine: Decoding PEM data...
I0119 12:31:16.295202   13136 main.go:141] libmachine: Parsing certificate...
I0119 12:31:16.297842   13136 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0119 12:31:16.336507   13136 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0119 12:31:16.336645   13136 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0119 12:31:16.336695   13136 cli_runner.go:164] Run: docker network inspect minikube
W0119 12:31:16.362002   13136 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0119 12:31:16.362041   13136 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0119 12:31:16.362058   13136 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0119 12:31:16.362399   13136 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0119 12:31:16.379955   13136 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14001db6110}
I0119 12:31:16.380189   13136 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0119 12:31:16.380406   13136 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0119 12:31:16.432515   13136 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0119 12:31:16.432979   13136 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0119 12:31:16.433094   13136 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0119 12:31:16.451430   13136 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0119 12:31:16.467313   13136 oci.go:103] Successfully created a docker volume minikube
I0119 12:31:16.467827   13136 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0119 12:31:16.866202   13136 oci.go:107] Successfully prepared a docker volume minikube
I0119 12:31:16.867584   13136 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0119 12:31:16.868133   13136 kic.go:194] Starting extracting preloaded images to volume ...
I0119 12:31:16.868290   13136 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/susheelchennoju/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0119 12:31:27.361557   13136 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/susheelchennoju/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (10.493288625s)
I0119 12:31:27.361685   13136 kic.go:203] duration metric: took 10.493684291s to extract preloaded images to volume ...
I0119 12:31:27.362331   13136 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0119 12:31:28.177010   13136 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=1988mb --memory-swap=1988mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0119 12:31:28.527802   13136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0119 12:31:28.548331   13136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 12:31:28.566308   13136 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0119 12:31:28.652578   13136 oci.go:144] the created container "minikube" has a running status.
I0119 12:31:28.652939   13136 kic.go:225] Creating ssh key for kic: /Users/susheelchennoju/.minikube/machines/minikube/id_rsa...
I0119 12:31:28.722946   13136 kic_runner.go:191] docker (temp): /Users/susheelchennoju/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0119 12:31:28.789050   13136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 12:31:28.807584   13136 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0119 12:31:28.807604   13136 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0119 12:31:28.891359   13136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 12:31:28.912891   13136 machine.go:93] provisionDockerMachine start ...
I0119 12:31:28.915566   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:28.936368   13136 main.go:141] libmachine: Using SSH client type: native
I0119 12:31:28.940115   13136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100ca0790] 0x100ca2fd0 <nil>  [] 0s} 127.0.0.1 56873 <nil> <nil>}
I0119 12:31:28.940123   13136 main.go:141] libmachine: About to run SSH command:
hostname
I0119 12:31:29.055822   13136 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0119 12:31:29.056114   13136 ubuntu.go:169] provisioning hostname "minikube"
I0119 12:31:29.056583   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:29.074762   13136 main.go:141] libmachine: Using SSH client type: native
I0119 12:31:29.074926   13136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100ca0790] 0x100ca2fd0 <nil>  [] 0s} 127.0.0.1 56873 <nil> <nil>}
I0119 12:31:29.074929   13136 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0119 12:31:29.196673   13136 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0119 12:31:29.196830   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:29.215125   13136 main.go:141] libmachine: Using SSH client type: native
I0119 12:31:29.215271   13136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100ca0790] 0x100ca2fd0 <nil>  [] 0s} 127.0.0.1 56873 <nil> <nil>}
I0119 12:31:29.215280   13136 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0119 12:31:29.323321   13136 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0119 12:31:29.323343   13136 ubuntu.go:175] set auth options {CertDir:/Users/susheelchennoju/.minikube CaCertPath:/Users/susheelchennoju/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/susheelchennoju/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/susheelchennoju/.minikube/machines/server.pem ServerKeyPath:/Users/susheelchennoju/.minikube/machines/server-key.pem ClientKeyPath:/Users/susheelchennoju/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/susheelchennoju/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/susheelchennoju/.minikube}
I0119 12:31:29.324212   13136 ubuntu.go:177] setting up certificates
I0119 12:31:29.324399   13136 provision.go:84] configureAuth start
I0119 12:31:29.324744   13136 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0119 12:31:29.345094   13136 provision.go:143] copyHostCerts
I0119 12:31:29.345489   13136 exec_runner.go:144] found /Users/susheelchennoju/.minikube/ca.pem, removing ...
I0119 12:31:29.345660   13136 exec_runner.go:203] rm: /Users/susheelchennoju/.minikube/ca.pem
I0119 12:31:29.345763   13136 exec_runner.go:151] cp: /Users/susheelchennoju/.minikube/certs/ca.pem --> /Users/susheelchennoju/.minikube/ca.pem (1103 bytes)
I0119 12:31:29.346028   13136 exec_runner.go:144] found /Users/susheelchennoju/.minikube/cert.pem, removing ...
I0119 12:31:29.346031   13136 exec_runner.go:203] rm: /Users/susheelchennoju/.minikube/cert.pem
I0119 12:31:29.346103   13136 exec_runner.go:151] cp: /Users/susheelchennoju/.minikube/certs/cert.pem --> /Users/susheelchennoju/.minikube/cert.pem (1143 bytes)
I0119 12:31:29.346375   13136 exec_runner.go:144] found /Users/susheelchennoju/.minikube/key.pem, removing ...
I0119 12:31:29.346378   13136 exec_runner.go:203] rm: /Users/susheelchennoju/.minikube/key.pem
I0119 12:31:29.346713   13136 exec_runner.go:151] cp: /Users/susheelchennoju/.minikube/certs/key.pem --> /Users/susheelchennoju/.minikube/key.pem (1679 bytes)
I0119 12:31:29.347089   13136 provision.go:117] generating server cert: /Users/susheelchennoju/.minikube/machines/server.pem ca-key=/Users/susheelchennoju/.minikube/certs/ca.pem private-key=/Users/susheelchennoju/.minikube/certs/ca-key.pem org=susheelchennoju.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0119 12:31:29.412854   13136 provision.go:177] copyRemoteCerts
I0119 12:31:29.413225   13136 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0119 12:31:29.413260   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:29.430317   13136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56873 SSHKeyPath:/Users/susheelchennoju/.minikube/machines/minikube/id_rsa Username:docker}
I0119 12:31:29.510385   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1103 bytes)
I0119 12:31:29.526944   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0119 12:31:29.541655   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0119 12:31:29.556564   13136 provision.go:87] duration metric: took 231.826334ms to configureAuth
I0119 12:31:29.556688   13136 ubuntu.go:193] setting minikube options for container-runtime
I0119 12:31:29.557944   13136 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0119 12:31:29.557992   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:29.574192   13136 main.go:141] libmachine: Using SSH client type: native
I0119 12:31:29.574337   13136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100ca0790] 0x100ca2fd0 <nil>  [] 0s} 127.0.0.1 56873 <nil> <nil>}
I0119 12:31:29.574341   13136 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0119 12:31:29.683655   13136 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0119 12:31:29.683668   13136 ubuntu.go:71] root file system type: overlay
I0119 12:31:29.685004   13136 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0119 12:31:29.685275   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:29.704049   13136 main.go:141] libmachine: Using SSH client type: native
I0119 12:31:29.704205   13136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100ca0790] 0x100ca2fd0 <nil>  [] 0s} 127.0.0.1 56873 <nil> <nil>}
I0119 12:31:29.704238   13136 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0119 12:31:29.823225   13136 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0119 12:31:29.823635   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:29.843932   13136 main.go:141] libmachine: Using SSH client type: native
I0119 12:31:29.844081   13136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x100ca0790] 0x100ca2fd0 <nil>  [] 0s} 127.0.0.1 56873 <nil> <nil>}
I0119 12:31:29.844087   13136 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0119 12:31:30.356668   13136 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:16.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-01-19 18:31:29.821477002 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0119 12:31:30.356711   13136 machine.go:96] duration metric: took 1.443805542s to provisionDockerMachine
I0119 12:31:30.357003   13136 client.go:171] duration metric: took 14.067343375s to LocalClient.Create
I0119 12:31:30.357051   13136 start.go:167] duration metric: took 14.068011917s to libmachine.API.Create "minikube"
I0119 12:31:30.357059   13136 start.go:293] postStartSetup for "minikube" (driver="docker")
I0119 12:31:30.357069   13136 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0119 12:31:30.357208   13136 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0119 12:31:30.357267   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:30.377438   13136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56873 SSHKeyPath:/Users/susheelchennoju/.minikube/machines/minikube/id_rsa Username:docker}
I0119 12:31:30.460710   13136 ssh_runner.go:195] Run: cat /etc/os-release
I0119 12:31:30.464052   13136 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0119 12:31:30.464085   13136 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0119 12:31:30.464097   13136 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0119 12:31:30.464102   13136 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0119 12:31:30.464341   13136 filesync.go:126] Scanning /Users/susheelchennoju/.minikube/addons for local assets ...
I0119 12:31:30.464491   13136 filesync.go:126] Scanning /Users/susheelchennoju/.minikube/files for local assets ...
I0119 12:31:30.464560   13136 start.go:296] duration metric: took 107.496208ms for postStartSetup
I0119 12:31:30.466331   13136 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0119 12:31:30.485149   13136 profile.go:143] Saving config to /Users/susheelchennoju/.minikube/profiles/minikube/config.json ...
I0119 12:31:30.485694   13136 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0119 12:31:30.485726   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:30.501487   13136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56873 SSHKeyPath:/Users/susheelchennoju/.minikube/machines/minikube/id_rsa Username:docker}
I0119 12:31:30.578387   13136 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0119 12:31:30.582343   13136 start.go:128] duration metric: took 14.313486875s to createHost
I0119 12:31:30.582530   13136 start.go:83] releasing machines lock for "minikube", held for 14.316968875s
I0119 12:31:30.582963   13136 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0119 12:31:30.604538   13136 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0119 12:31:30.604820   13136 ssh_runner.go:195] Run: cat /version.json
I0119 12:31:30.604864   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:30.604878   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:30.624737   13136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56873 SSHKeyPath:/Users/susheelchennoju/.minikube/machines/minikube/id_rsa Username:docker}
I0119 12:31:30.624769   13136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56873 SSHKeyPath:/Users/susheelchennoju/.minikube/machines/minikube/id_rsa Username:docker}
I0119 12:31:30.882222   13136 ssh_runner.go:195] Run: systemctl --version
I0119 12:31:30.886686   13136 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0119 12:31:30.891169   13136 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0119 12:31:30.908449   13136 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0119 12:31:30.908744   13136 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0119 12:31:30.925567   13136 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0119 12:31:30.926103   13136 start.go:495] detecting cgroup driver to use...
I0119 12:31:30.926124   13136 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0119 12:31:30.927112   13136 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0119 12:31:30.937432   13136 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0119 12:31:30.944213   13136 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0119 12:31:30.951927   13136 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0119 12:31:30.952009   13136 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0119 12:31:30.958525   13136 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0119 12:31:30.965232   13136 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0119 12:31:30.971780   13136 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0119 12:31:30.978094   13136 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0119 12:31:30.984716   13136 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0119 12:31:30.991294   13136 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0119 12:31:30.998156   13136 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0119 12:31:31.004595   13136 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0119 12:31:31.010679   13136 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0119 12:31:31.016461   13136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 12:31:31.073345   13136 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0119 12:31:31.146074   13136 start.go:495] detecting cgroup driver to use...
I0119 12:31:31.146097   13136 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0119 12:31:31.146385   13136 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0119 12:31:31.155442   13136 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0119 12:31:31.155559   13136 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0119 12:31:31.163691   13136 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0119 12:31:31.174688   13136 ssh_runner.go:195] Run: which cri-dockerd
I0119 12:31:31.177834   13136 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0119 12:31:31.184272   13136 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0119 12:31:31.196507   13136 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0119 12:31:31.247471   13136 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0119 12:31:31.297870   13136 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0119 12:31:31.300263   13136 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0119 12:31:31.311693   13136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 12:31:31.372535   13136 ssh_runner.go:195] Run: sudo systemctl restart docker
I0119 12:31:31.554593   13136 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0119 12:31:31.563332   13136 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0119 12:31:31.571964   13136 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0119 12:31:31.626400   13136 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0119 12:31:31.678129   13136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 12:31:31.736050   13136 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0119 12:31:31.753893   13136 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0119 12:31:31.762049   13136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 12:31:31.817305   13136 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0119 12:31:31.952957   13136 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0119 12:31:31.953570   13136 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0119 12:31:31.958068   13136 start.go:563] Will wait 60s for crictl version
I0119 12:31:31.958216   13136 ssh_runner.go:195] Run: which crictl
I0119 12:31:31.962202   13136 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0119 12:31:32.040390   13136 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0119 12:31:32.040494   13136 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0119 12:31:32.100972   13136 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0119 12:31:32.123862   13136 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0119 12:31:32.124733   13136 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0119 12:31:32.257212   13136 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0119 12:31:32.257362   13136 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0119 12:31:32.261771   13136 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0119 12:31:32.270022   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0119 12:31:32.288970   13136 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:1988 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0119 12:31:32.290078   13136 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0119 12:31:32.290140   13136 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0119 12:31:32.305450   13136 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0119 12:31:32.305461   13136 docker.go:619] Images already preloaded, skipping extraction
I0119 12:31:32.305770   13136 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0119 12:31:32.320007   13136 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0119 12:31:32.320021   13136 cache_images.go:84] Images are preloaded, skipping loading
I0119 12:31:32.320025   13136 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0119 12:31:32.320678   13136 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0119 12:31:32.320738   13136 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0119 12:31:32.376879   13136 cni.go:84] Creating CNI manager for ""
I0119 12:31:32.376899   13136 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0119 12:31:32.376928   13136 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0119 12:31:32.377762   13136 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0119 12:31:32.378162   13136 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0119 12:31:32.378305   13136 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0119 12:31:32.386559   13136 binaries.go:44] Found k8s binaries, skipping transfer
I0119 12:31:32.386637   13136 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0119 12:31:32.392776   13136 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0119 12:31:32.404114   13136 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0119 12:31:32.415096   13136 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0119 12:31:32.426184   13136 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0119 12:31:32.429737   13136 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0119 12:31:32.437279   13136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 12:31:32.509174   13136 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0119 12:31:32.533859   13136 certs.go:68] Setting up /Users/susheelchennoju/.minikube/profiles/minikube for IP: 192.168.49.2
I0119 12:31:32.533869   13136 certs.go:194] generating shared ca certs ...
I0119 12:31:32.533886   13136 certs.go:226] acquiring lock for ca certs: {Name:mk3a1f77c26d6c5f73c085ef32c15b6f11fdee0e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:32.535278   13136 certs.go:240] generating "minikubeCA" ca cert: /Users/susheelchennoju/.minikube/ca.key
I0119 12:31:32.726516   13136 crypto.go:156] Writing cert to /Users/susheelchennoju/.minikube/ca.crt ...
I0119 12:31:32.726524   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/ca.crt: {Name:mk09016ea41d8558b7dd96700b356b6b0f75abcb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:32.726848   13136 crypto.go:164] Writing key to /Users/susheelchennoju/.minikube/ca.key ...
I0119 12:31:32.726852   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/ca.key: {Name:mk3d55f9c444c6ec9309ceaf494b876773ade388 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:32.727001   13136 certs.go:240] generating "proxyClientCA" ca cert: /Users/susheelchennoju/.minikube/proxy-client-ca.key
I0119 12:31:32.921842   13136 crypto.go:156] Writing cert to /Users/susheelchennoju/.minikube/proxy-client-ca.crt ...
I0119 12:31:32.921851   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/proxy-client-ca.crt: {Name:mk4066d9d84e2e2557c4f240e66589724d1d5ed7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:32.922202   13136 crypto.go:164] Writing key to /Users/susheelchennoju/.minikube/proxy-client-ca.key ...
I0119 12:31:32.922204   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/proxy-client-ca.key: {Name:mk8046a26a7cb1bced9c2e30671e8839220bcab7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:32.922950   13136 certs.go:256] generating profile certs ...
I0119 12:31:32.922989   13136 certs.go:363] generating signed profile cert for "minikube-user": /Users/susheelchennoju/.minikube/profiles/minikube/client.key
I0119 12:31:32.923189   13136 crypto.go:68] Generating cert /Users/susheelchennoju/.minikube/profiles/minikube/client.crt with IP's: []
I0119 12:31:33.032806   13136 crypto.go:156] Writing cert to /Users/susheelchennoju/.minikube/profiles/minikube/client.crt ...
I0119 12:31:33.032811   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/profiles/minikube/client.crt: {Name:mkf0aa2d854a5240ec23e49ba1b5c330d63801b4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:33.033033   13136 crypto.go:164] Writing key to /Users/susheelchennoju/.minikube/profiles/minikube/client.key ...
I0119 12:31:33.033036   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/profiles/minikube/client.key: {Name:mk0e479c7df7742ee01d9e261409437c6b8d7f3a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:33.033164   13136 certs.go:363] generating signed profile cert for "minikube": /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0119 12:31:33.033174   13136 crypto.go:68] Generating cert /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0119 12:31:33.104044   13136 crypto.go:156] Writing cert to /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0119 12:31:33.104046   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk647f555a9fc3cb83234792aa271b3379ca0737 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:33.104201   13136 crypto.go:164] Writing key to /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0119 12:31:33.104203   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk9c874873f108a74503385ee83596b86712fedc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:33.104322   13136 certs.go:381] copying /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.crt
I0119 12:31:33.105090   13136 certs.go:385] copying /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.key
I0119 12:31:33.105189   13136 certs.go:363] generating signed profile cert for "aggregator": /Users/susheelchennoju/.minikube/profiles/minikube/proxy-client.key
I0119 12:31:33.105199   13136 crypto.go:68] Generating cert /Users/susheelchennoju/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0119 12:31:33.166201   13136 crypto.go:156] Writing cert to /Users/susheelchennoju/.minikube/profiles/minikube/proxy-client.crt ...
I0119 12:31:33.166213   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/profiles/minikube/proxy-client.crt: {Name:mkb4f6c0f2a5811feab644969c38a4febffc76b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:33.166511   13136 crypto.go:164] Writing key to /Users/susheelchennoju/.minikube/profiles/minikube/proxy-client.key ...
I0119 12:31:33.166514   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.minikube/profiles/minikube/proxy-client.key: {Name:mk495976413bd9dbec900acb40c55ee26776ae91 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:33.167194   13136 certs.go:484] found cert: /Users/susheelchennoju/.minikube/certs/ca-key.pem (1679 bytes)
I0119 12:31:33.167364   13136 certs.go:484] found cert: /Users/susheelchennoju/.minikube/certs/ca.pem (1103 bytes)
I0119 12:31:33.167386   13136 certs.go:484] found cert: /Users/susheelchennoju/.minikube/certs/cert.pem (1143 bytes)
I0119 12:31:33.167497   13136 certs.go:484] found cert: /Users/susheelchennoju/.minikube/certs/key.pem (1679 bytes)
I0119 12:31:33.177082   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0119 12:31:33.221713   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0119 12:31:33.242677   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0119 12:31:33.261524   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0119 12:31:33.276649   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0119 12:31:33.293058   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0119 12:31:33.307335   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0119 12:31:33.321632   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0119 12:31:33.336196   13136 ssh_runner.go:362] scp /Users/susheelchennoju/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0119 12:31:33.351356   13136 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0119 12:31:33.364407   13136 ssh_runner.go:195] Run: openssl version
I0119 12:31:33.371881   13136 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0119 12:31:33.379391   13136 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0119 12:31:33.382650   13136 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jan 19 18:31 /usr/share/ca-certificates/minikubeCA.pem
I0119 12:31:33.382709   13136 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0119 12:31:33.388044   13136 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0119 12:31:33.394608   13136 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0119 12:31:33.397940   13136 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0119 12:31:33.398018   13136 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:1988 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0119 12:31:33.398138   13136 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0119 12:31:33.413830   13136 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0119 12:31:33.420449   13136 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0119 12:31:33.426803   13136 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0119 12:31:33.427152   13136 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0119 12:31:33.433465   13136 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0119 12:31:33.433801   13136 kubeadm.go:157] found existing configuration files:

I0119 12:31:33.433934   13136 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0119 12:31:33.440739   13136 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0119 12:31:33.440880   13136 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0119 12:31:33.447563   13136 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0119 12:31:33.453828   13136 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0119 12:31:33.453967   13136 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0119 12:31:33.461001   13136 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0119 12:31:33.467462   13136 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0119 12:31:33.467563   13136 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0119 12:31:33.473153   13136 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0119 12:31:33.479364   13136 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0119 12:31:33.479438   13136 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0119 12:31:33.486517   13136 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0119 12:31:33.600839   13136 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0119 12:31:33.600911   13136 kubeadm.go:310] [preflight] Running pre-flight checks
I0119 12:31:33.675841   13136 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0119 12:31:33.676003   13136 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0119 12:31:33.676145   13136 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0119 12:31:33.685321   13136 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0119 12:31:33.693707   13136 out.go:235]     ▪ Generating certificates and keys ...
I0119 12:31:33.693942   13136 kubeadm.go:310] [certs] Using existing ca certificate authority
I0119 12:31:33.694138   13136 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0119 12:31:33.854881   13136 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0119 12:31:33.917478   13136 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0119 12:31:33.961292   13136 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0119 12:31:34.055414   13136 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0119 12:31:34.136542   13136 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0119 12:31:34.136708   13136 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0119 12:31:34.195805   13136 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0119 12:31:34.195978   13136 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0119 12:31:34.267251   13136 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0119 12:31:34.407083   13136 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0119 12:31:34.547964   13136 kubeadm.go:310] [certs] Generating "sa" key and public key
I0119 12:31:34.548120   13136 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0119 12:31:34.655502   13136 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0119 12:31:34.788542   13136 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0119 12:31:34.908463   13136 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0119 12:31:34.961398   13136 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0119 12:31:35.084567   13136 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0119 12:31:35.084835   13136 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0119 12:31:35.088198   13136 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0119 12:31:35.092083   13136 out.go:235]     ▪ Booting up control plane ...
I0119 12:31:35.092240   13136 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0119 12:31:35.092344   13136 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0119 12:31:35.092448   13136 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0119 12:31:35.095590   13136 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0119 12:31:35.100171   13136 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0119 12:31:35.100216   13136 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0119 12:31:35.168074   13136 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0119 12:31:35.168249   13136 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0119 12:31:35.670757   13136 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.862583ms
I0119 12:31:35.670901   13136 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0119 12:31:38.671392   13136 kubeadm.go:310] [api-check] The API server is healthy after 3.001012044s
I0119 12:31:38.677595   13136 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0119 12:31:38.683650   13136 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0119 12:31:38.692553   13136 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0119 12:31:38.692800   13136 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0119 12:31:38.696677   13136 kubeadm.go:310] [bootstrap-token] Using token: r05tsd.rzk3gbd694u46djp
I0119 12:31:38.700716   13136 out.go:235]     ▪ Configuring RBAC rules ...
I0119 12:31:38.700887   13136 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0119 12:31:38.702306   13136 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0119 12:31:38.709263   13136 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0119 12:31:38.710673   13136 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0119 12:31:38.711854   13136 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0119 12:31:38.713861   13136 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0119 12:31:39.085247   13136 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0119 12:31:39.488829   13136 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0119 12:31:40.077133   13136 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0119 12:31:40.077737   13136 kubeadm.go:310] 
I0119 12:31:40.077820   13136 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0119 12:31:40.077827   13136 kubeadm.go:310] 
I0119 12:31:40.077962   13136 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0119 12:31:40.077973   13136 kubeadm.go:310] 
I0119 12:31:40.078012   13136 kubeadm.go:310]   mkdir -p $HOME/.kube
I0119 12:31:40.078107   13136 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0119 12:31:40.078183   13136 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0119 12:31:40.078192   13136 kubeadm.go:310] 
I0119 12:31:40.078259   13136 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0119 12:31:40.078263   13136 kubeadm.go:310] 
I0119 12:31:40.078325   13136 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0119 12:31:40.078329   13136 kubeadm.go:310] 
I0119 12:31:40.078408   13136 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0119 12:31:40.078520   13136 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0119 12:31:40.078621   13136 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0119 12:31:40.078626   13136 kubeadm.go:310] 
I0119 12:31:40.078752   13136 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0119 12:31:40.078867   13136 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0119 12:31:40.078877   13136 kubeadm.go:310] 
I0119 12:31:40.079002   13136 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token r05tsd.rzk3gbd694u46djp \
I0119 12:31:40.079157   13136 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:f0a2e9e4b3eae3df9a68efb4d70d146abf816ca0bdb06046ed099b791baa9636 \
I0119 12:31:40.079187   13136 kubeadm.go:310] 	--control-plane 
I0119 12:31:40.079191   13136 kubeadm.go:310] 
I0119 12:31:40.079302   13136 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0119 12:31:40.079305   13136 kubeadm.go:310] 
I0119 12:31:40.079431   13136 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token r05tsd.rzk3gbd694u46djp \
I0119 12:31:40.079598   13136 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:f0a2e9e4b3eae3df9a68efb4d70d146abf816ca0bdb06046ed099b791baa9636 
I0119 12:31:40.084146   13136 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0119 12:31:40.084298   13136 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0119 12:31:40.084323   13136 cni.go:84] Creating CNI manager for ""
I0119 12:31:40.084347   13136 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0119 12:31:40.092947   13136 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0119 12:31:40.097025   13136 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0119 12:31:40.108553   13136 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0119 12:31:40.120484   13136 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0119 12:31:40.120633   13136 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0119 12:31:40.121362   13136 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_01_19T12_31_40_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0119 12:31:40.271663   13136 kubeadm.go:1113] duration metric: took 151.16275ms to wait for elevateKubeSystemPrivileges
I0119 12:31:40.271698   13136 ops.go:34] apiserver oom_adj: -16
I0119 12:31:40.272025   13136 kubeadm.go:394] duration metric: took 6.874109291s to StartCluster
I0119 12:31:40.272053   13136 settings.go:142] acquiring lock: {Name:mk2e408f2ced4256ac6fa362d6805fdebb4bff58 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:40.272379   13136 settings.go:150] Updating kubeconfig:  /Users/susheelchennoju/.kube/config
I0119 12:31:40.272868   13136 lock.go:35] WriteFile acquiring /Users/susheelchennoju/.kube/config: {Name:mk8626481592dbfb1fc30ff7dc85e91651c6b1f2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0119 12:31:40.273847   13136 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0119 12:31:40.274053   13136 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0119 12:31:40.274076   13136 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0119 12:31:40.273864   13136 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0119 12:31:40.274391   13136 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0119 12:31:40.274406   13136 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0119 12:31:40.274401   13136 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0119 12:31:40.274783   13136 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0119 12:31:40.274939   13136 host.go:66] Checking if "minikube" exists ...
I0119 12:31:40.276688   13136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 12:31:40.276839   13136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 12:31:40.279232   13136 out.go:177] 🔎  Verifying Kubernetes components...
I0119 12:31:40.287956   13136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0119 12:31:40.396049   13136 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0119 12:31:40.400094   13136 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0119 12:31:40.400102   13136 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0119 12:31:40.400162   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:40.406712   13136 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0119 12:31:40.406753   13136 host.go:66] Checking if "minikube" exists ...
I0119 12:31:40.406990   13136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0119 12:31:40.424610   13136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56873 SSHKeyPath:/Users/susheelchennoju/.minikube/machines/minikube/id_rsa Username:docker}
I0119 12:31:40.427096   13136 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0119 12:31:40.430428   13136 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0119 12:31:40.430440   13136 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0119 12:31:40.430643   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0119 12:31:40.437913   13136 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0119 12:31:40.451244   13136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56873 SSHKeyPath:/Users/susheelchennoju/.minikube/machines/minikube/id_rsa Username:docker}
I0119 12:31:40.554047   13136 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0119 12:31:40.580600   13136 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0119 12:31:40.596768   13136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0119 12:31:40.596831   13136 start.go:971] {"host.minikube.internal": 192.168.65.2} host record injected into CoreDNS's ConfigMap
I0119 12:31:40.614538   13136 api_server.go:52] waiting for apiserver process to appear ...
I0119 12:31:40.614630   13136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0119 12:31:40.785658   13136 api_server.go:72] duration metric: took 511.561666ms to wait for apiserver process to appear ...
I0119 12:31:40.785677   13136 api_server.go:88] waiting for apiserver healthz status ...
I0119 12:31:40.785696   13136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56872/healthz ...
I0119 12:31:40.791612   13136 api_server.go:279] https://127.0.0.1:56872/healthz returned 200:
ok
I0119 12:31:40.793930   13136 api_server.go:141] control plane version: v1.32.0
I0119 12:31:40.793944   13136 api_server.go:131] duration metric: took 8.262125ms to wait for apiserver health ...
I0119 12:31:40.794104   13136 system_pods.go:43] waiting for kube-system pods to appear ...
I0119 12:31:40.797515   13136 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0119 12:31:40.802567   13136 system_pods.go:59] 5 kube-system pods found
I0119 12:31:40.802583   13136 system_pods.go:61] "etcd-minikube" [b9704e4d-971f-46ab-a5b1-b3539bbba1b2] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0119 12:31:40.802586   13136 system_pods.go:61] "kube-apiserver-minikube" [85980140-bef8-4404-97df-0f247bb9eca6] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0119 12:31:40.802593   13136 system_pods.go:61] "kube-controller-manager-minikube" [571b5dd4-4e80-489d-9dee-63dff9edbf03] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0119 12:31:40.802595   13136 system_pods.go:61] "kube-scheduler-minikube" [59c595a8-835d-4d1a-a294-5fdad11907e4] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0119 12:31:40.802596   13136 system_pods.go:61] "storage-provisioner" [e3f5af73-210f-4708-84e5-d24575588e28] Pending
I0119 12:31:40.802599   13136 system_pods.go:74] duration metric: took 8.492125ms to wait for pod list to return data ...
I0119 12:31:40.802603   13136 kubeadm.go:582] duration metric: took 528.516916ms to wait for: map[apiserver:true system_pods:true]
I0119 12:31:40.802612   13136 node_conditions.go:102] verifying NodePressure condition ...
I0119 12:31:40.805240   13136 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I0119 12:31:40.805262   13136 addons.go:514] duration metric: took 531.58475ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0119 12:31:40.805376   13136 node_conditions.go:123] node cpu capacity is 4
I0119 12:31:40.805382   13136 node_conditions.go:105] duration metric: took 2.768125ms to run NodePressure ...
I0119 12:31:40.805387   13136 start.go:241] waiting for startup goroutines ...
I0119 12:31:41.102046   13136 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0119 12:31:41.102074   13136 start.go:246] waiting for cluster config update ...
I0119 12:31:41.102089   13136 start.go:255] writing updated cluster config ...
I0119 12:31:41.103873   13136 ssh_runner.go:195] Run: rm -f paused
I0119 12:31:41.293529   13136 start.go:600] kubectl: 1.32.1, cluster: 1.32.0 (minor skew: 0)
I0119 12:31:41.298238   13136 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jan 19 18:37:08 minikube dockerd[1240]: time="2025-01-19T18:37:08.987445798Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 19 18:37:37 minikube dockerd[1240]: time="2025-01-19T18:37:37.148064047Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 19 18:37:37 minikube dockerd[1240]: time="2025-01-19T18:37:37.148282839Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 19 18:38:27 minikube dockerd[1240]: time="2025-01-19T18:38:27.120294292Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 19 18:38:27 minikube dockerd[1240]: time="2025-01-19T18:38:27.120587000Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 19 18:40:01 minikube dockerd[1240]: time="2025-01-19T18:40:01.955536711Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 19 18:40:01 minikube dockerd[1240]: time="2025-01-19T18:40:01.955825919Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 19 18:42:45 minikube dockerd[1240]: time="2025-01-19T18:42:45.932236634Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 19 18:42:45 minikube dockerd[1240]: time="2025-01-19T18:42:45.932358968Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 19 18:47:08 minikube dockerd[1240]: time="2025-01-19T18:47:08.340663006Z" level=info msg="ignoring event" container=75a34a7a58a4650d56bf46565f690d875795690d002e28f7851a308c1e2af7d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 18:48:11 minikube cri-dockerd[1517]: time="2025-01-19T18:48:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fcf15801eab860aa990363476177ff94f313d48a4de0baa462432ac38327d2c2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 18:48:16 minikube cri-dockerd[1517]: time="2025-01-19T18:48:16Z" level=info msg="Stop pulling image nginx:latest: Status: Downloaded newer image for nginx:latest"
Jan 19 19:11:07 minikube cri-dockerd[1517]: time="2025-01-19T19:11:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fad07af1239fc967ba22db43c3dbff05416c6781a1ab9a5e96e65fa84de7ed0b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 19:11:18 minikube cri-dockerd[1517]: time="2025-01-19T19:11:18Z" level=info msg="Pulling image mongo:latest: da089b883a54: Extracting [================================>                  ]  148.7MB/232.1MB"
Jan 19 19:11:21 minikube cri-dockerd[1517]: time="2025-01-19T19:11:21Z" level=info msg="Stop pulling image mongo:latest: Status: Downloaded newer image for mongo:latest"
Jan 19 19:31:56 minikube dockerd[1240]: time="2025-01-19T19:31:56.303040222Z" level=info msg="ignoring event" container=6d05764862cce6f03ca6c790a63823e7f368de8bfd5cb1f71e109ea124e02555 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 19:31:56 minikube dockerd[1240]: time="2025-01-19T19:31:56.423001430Z" level=info msg="ignoring event" container=fad07af1239fc967ba22db43c3dbff05416c6781a1ab9a5e96e65fa84de7ed0b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 19:42:49 minikube cri-dockerd[1517]: time="2025-01-19T19:42:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f6a149bc18ebc7926cb013c2afda4b45911321f4d803b806404d7599a27fedb5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 19:42:49 minikube cri-dockerd[1517]: time="2025-01-19T19:42:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/21ef780afc99662c169aa2a50400ae7801b4ef0b0f0ea8e596ff449ad03499f5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 19:42:50 minikube cri-dockerd[1517]: time="2025-01-19T19:42:50Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jan 19 19:42:50 minikube cri-dockerd[1517]: time="2025-01-19T19:42:50Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jan 19 19:44:31 minikube dockerd[1240]: time="2025-01-19T19:44:31.833669919Z" level=info msg="ignoring event" container=eaea566563c099035e5e4464d01d6878e7620a029dd6d2d481cee1d0787bb13f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 19:44:31 minikube dockerd[1240]: time="2025-01-19T19:44:31.941678752Z" level=info msg="ignoring event" container=fcf15801eab860aa990363476177ff94f313d48a4de0baa462432ac38327d2c2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 19:51:57 minikube dockerd[1240]: time="2025-01-19T19:51:57.453610167Z" level=info msg="ignoring event" container=0b5580fa5ec1b70a4d831dfe1576eb57484435d36a73d818899fa17c71872eaa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 19:51:57 minikube dockerd[1240]: time="2025-01-19T19:51:57.454268958Z" level=info msg="ignoring event" container=a7363390a53fa6a2d9494824f7a089bc7a01753cb5afa1af821e21aba53d77b6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 19:51:57 minikube dockerd[1240]: time="2025-01-19T19:51:57.547918583Z" level=info msg="ignoring event" container=f6a149bc18ebc7926cb013c2afda4b45911321f4d803b806404d7599a27fedb5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 19:51:57 minikube dockerd[1240]: time="2025-01-19T19:51:57.549232167Z" level=info msg="ignoring event" container=21ef780afc99662c169aa2a50400ae7801b4ef0b0f0ea8e596ff449ad03499f5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 20:06:10 minikube cri-dockerd[1517]: time="2025-01-19T20:06:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f1993ef736a1006c4cc2f52772b036bfc29e0221ebc11441b743ecf57611d029/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 20:06:10 minikube cri-dockerd[1517]: time="2025-01-19T20:06:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2dd416cbc05aec9b70d64d41c89e27d1f4d19aa557881a82ebe659f64a774e51/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 20:06:11 minikube cri-dockerd[1517]: time="2025-01-19T20:06:11Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jan 19 20:06:11 minikube cri-dockerd[1517]: time="2025-01-19T20:06:11Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Jan 19 20:26:19 minikube dockerd[1240]: time="2025-01-19T20:26:19.874281260Z" level=info msg="ignoring event" container=825c98c4da3a9afa2fa7e470a6b2afa29fb9589de295ff284fa4c21a2e8afad0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 20:26:19 minikube dockerd[1240]: time="2025-01-19T20:26:19.875281510Z" level=info msg="ignoring event" container=df3646dcb450fd07062c3ab031d08a2cb5b3e4df74b08e8cfe2895f6668849a2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 20:26:19 minikube dockerd[1240]: time="2025-01-19T20:26:19.984869843Z" level=info msg="ignoring event" container=f1993ef736a1006c4cc2f52772b036bfc29e0221ebc11441b743ecf57611d029 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 20:26:19 minikube dockerd[1240]: time="2025-01-19T20:26:19.993244718Z" level=info msg="ignoring event" container=2dd416cbc05aec9b70d64d41c89e27d1f4d19aa557881a82ebe659f64a774e51 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 21:00:18 minikube dockerd[1240]: time="2025-01-19T21:00:18.645788717Z" level=info msg="ignoring event" container=52a8876e2266f0fe97476cb63428e083ba4c354956f4765c425ee7a04b90f834 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 21:00:21 minikube cri-dockerd[1517]: time="2025-01-19T21:00:21Z" level=error msg="error getting RW layer size for container ID '18a8dcb937a2e3372977831819ff15ac53e2db55c51efeb3a222e4e110a7155d': Error response from daemon: No such container: 18a8dcb937a2e3372977831819ff15ac53e2db55c51efeb3a222e4e110a7155d"
Jan 19 21:00:21 minikube cri-dockerd[1517]: time="2025-01-19T21:00:21Z" level=error msg="Set backoffDuration to : 1m0s for container ID '18a8dcb937a2e3372977831819ff15ac53e2db55c51efeb3a222e4e110a7155d'"
Jan 19 21:40:33 minikube cri-dockerd[1517]: time="2025-01-19T21:40:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/368860847cfbfe9d3a2f88689c4e8f03d0f68e0a1051213f480e2f8b416246fb/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 21:40:34 minikube cri-dockerd[1517]: time="2025-01-19T21:40:34Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jan 19 22:17:57 minikube cri-dockerd[1517]: time="2025-01-19T22:17:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68fe76eb193dc91d17194b5bb6b65861ec8296e64efe04f1d72c79523366b31c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 19 22:18:05 minikube cri-dockerd[1517]: time="2025-01-19T22:18:05Z" level=info msg="Stop pulling image mongo-express:latest: Status: Downloaded newer image for mongo-express:latest"
Jan 19 22:18:45 minikube dockerd[1240]: time="2025-01-19T22:18:45.713590134Z" level=info msg="ignoring event" container=ebe4f4087c982d544a69a16fee56ed027e8133b53666e2728c2601374f12e9da module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 22:18:47 minikube cri-dockerd[1517]: time="2025-01-19T22:18:47Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jan 19 22:19:27 minikube dockerd[1240]: time="2025-01-19T22:19:27.224716459Z" level=info msg="ignoring event" container=958a6b0ca487f1ea3aea1680ed90bf917ce475661303caf592374231bc105f44 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 22:19:40 minikube cri-dockerd[1517]: time="2025-01-19T22:19:40Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jan 19 22:20:19 minikube dockerd[1240]: time="2025-01-19T22:20:19.976285220Z" level=info msg="ignoring event" container=5aa4dc9a6f9c652f65acee7450b46a0c37340a88cfbb6acc8c9e29146535dc5f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 22:20:47 minikube cri-dockerd[1517]: time="2025-01-19T22:20:47Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jan 19 22:21:26 minikube dockerd[1240]: time="2025-01-19T22:21:26.896033042Z" level=info msg="ignoring event" container=bf041d87f28eb6cdec709aa9a21012cb8ee713a3741592886b6c313dea0dde77 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 22:22:14 minikube cri-dockerd[1517]: time="2025-01-19T22:22:14Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jan 19 22:22:54 minikube dockerd[1240]: time="2025-01-19T22:22:54.081137597Z" level=info msg="ignoring event" container=3d603ef778a44c5fa1d8162a76a73a0ecc1c756c8f67a44a527be3c0e46e3020 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 22:24:21 minikube cri-dockerd[1517]: time="2025-01-19T22:24:21Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jan 19 22:25:02 minikube dockerd[1240]: time="2025-01-19T22:25:02.154904419Z" level=info msg="ignoring event" container=d53888300007a2adae33663a280fc3ef5058ccf4da97bb345daf29c6c5932556 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 22:27:53 minikube cri-dockerd[1517]: time="2025-01-19T22:27:53Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jan 19 22:28:33 minikube dockerd[1240]: time="2025-01-19T22:28:33.871983712Z" level=info msg="ignoring event" container=d17ecbe0f0ccff3732acd885c327fbafe5a052c861c1b7cbc79e16947e71cf73 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 22:33:35 minikube cri-dockerd[1517]: time="2025-01-19T22:33:35Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jan 19 22:34:15 minikube dockerd[1240]: time="2025-01-19T22:34:15.805388801Z" level=info msg="ignoring event" container=b3b5e6c38c8237178faea09e191dbebdc8c0e483e4383b57d96846183bbc61cf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 22:39:24 minikube cri-dockerd[1517]: time="2025-01-19T22:39:24Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jan 19 22:40:04 minikube dockerd[1240]: time="2025-01-19T22:40:04.986697837Z" level=info msg="ignoring event" container=0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 19 22:45:11 minikube cri-dockerd[1517]: time="2025-01-19T22:45:11Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"


==> container status <==
CONTAINER           IMAGE                                                                                   CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
d68d950713ac3       mongo-express@sha256:1b23d7976f0210dbec74045c209e52fbb26d29b2e873d6c6fa3d3f0ae32c2a64   4 hours ago         Running             mongo-express             9                   68fe76eb193dc       mongo-express-5dd87b9fcf-5j522
0cf4eecf1d065       mongo-express@sha256:1b23d7976f0210dbec74045c209e52fbb26d29b2e873d6c6fa3d3f0ae32c2a64   5 hours ago         Exited              mongo-express             8                   68fe76eb193dc       mongo-express-5dd87b9fcf-5j522
7c136235de64f       mongo@sha256:33d17eb9a1c27c6635d08f890cd075d0f9392c598eaad8c4461344ac5fbc7140           5 hours ago         Running             mongodb                   0                   368860847cfbf       my-name-mongodb-deployment-5b8c785c-ck8zv
656d3fc6120c4       ba04bb24b9575                                                                           6 hours ago         Running             storage-provisioner       2                   6f465ceccb33f       storage-provisioner
52a8876e2266f       ba04bb24b9575                                                                           9 hours ago         Exited              storage-provisioner       1                   6f465ceccb33f       storage-provisioner
f9412c6b97e35       2f6c962e7b831                                                                           9 hours ago         Running             coredns                   0                   2eb2b325a3dca       coredns-668d6bf9bc-b6hwf
e7664f2707e60       2f50386e20bfd                                                                           9 hours ago         Running             kube-proxy                0                   420c011b04f15       kube-proxy-nj4hn
f2df69cd17637       2b5bd0f16085a                                                                           9 hours ago         Running             kube-apiserver            0                   cdc963a358a20       kube-apiserver-minikube
69a8638dafce0       a8d049396f6b8                                                                           9 hours ago         Running             kube-controller-manager   0                   33791fd632946       kube-controller-manager-minikube
1c515e97750d5       c3ff26fb59f37                                                                           9 hours ago         Running             kube-scheduler            0                   a5e517e948efd       kube-scheduler-minikube
d1b9f27afd048       7fc9d4aa817aa                                                                           9 hours ago         Running             etcd                      0                   522dccb3233b4       etcd-minikube


==> coredns [f9412c6b97e3] <==
[INFO] 10.244.0.11:54484 - 60763 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000112875s
[INFO] 10.244.0.11:54484 - 60472 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000388292s
[INFO] 10.244.0.11:52446 - 22468 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000106333s
[INFO] 10.244.0.11:52446 - 22260 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000091708s
[INFO] 10.244.0.11:35740 - 29147 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.0069965s
[INFO] 10.244.0.11:35740 - 29272 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.007173583s
[INFO] 10.244.0.11:51232 - 1105 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000486291s
[INFO] 10.244.0.11:51232 - 480 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000546958s
[INFO] 10.244.0.11:51248 - 62794 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.0001655s
[INFO] 10.244.0.11:51248 - 62294 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000506s
[INFO] 10.244.0.11:47536 - 5636 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000561583s
[INFO] 10.244.0.11:47536 - 5219 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000348333s
[INFO] 10.244.0.11:46491 - 62381 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.008954s
[INFO] 10.244.0.11:46491 - 62089 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.009345542s
[INFO] 10.244.0.11:40970 - 40693 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000399166s
[INFO] 10.244.0.11:40970 - 41151 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000646709s
[INFO] 10.244.0.11:55082 - 30958 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000137583s
[INFO] 10.244.0.11:55082 - 31249 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000458125s
[INFO] 10.244.0.11:39910 - 51816 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000102167s
[INFO] 10.244.0.11:39910 - 52024 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000142208s
[INFO] 10.244.0.11:34485 - 1097 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.007405333s
[INFO] 10.244.0.11:34485 - 1519 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.007638625s
[INFO] 10.244.0.11:50054 - 49874 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000156542s
[INFO] 10.244.0.11:50054 - 49410 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.00009225s
[INFO] 10.244.0.11:54760 - 65121 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000104708s
[INFO] 10.244.0.11:54760 - 64782 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000082s
[INFO] 10.244.0.11:36787 - 7280 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000096625s
[INFO] 10.244.0.11:36787 - 7155 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.0000575s
[INFO] 10.244.0.11:56142 - 28666 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.014930916s
[INFO] 10.244.0.11:56142 - 28327 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.016179125s
[INFO] 10.244.0.11:52971 - 14583 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000119792s
[INFO] 10.244.0.11:52971 - 14160 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000044959s
[INFO] 10.244.0.11:53297 - 27040 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000076667s
[INFO] 10.244.0.11:53297 - 26700 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000040959s
[INFO] 10.244.0.11:35847 - 17019 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000064667s
[INFO] 10.244.0.11:35847 - 16852 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000063333s
[INFO] 10.244.0.11:56359 - 39789 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.005627291s
[INFO] 10.244.0.11:56359 - 39664 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.005642709s
[INFO] 10.244.0.11:52440 - 55537 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000158292s
[INFO] 10.244.0.11:52440 - 55704 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000310875s
[INFO] 10.244.0.11:59242 - 35485 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000071042s
[INFO] 10.244.0.11:59242 - 35104 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000087916s
[INFO] 10.244.0.11:34798 - 12832 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.00004475s
[INFO] 10.244.0.11:34798 - 12624 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000066417s
[INFO] 10.244.0.11:45751 - 1871 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.00535375s
[INFO] 10.244.0.11:45751 - 1573 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.005270041s
[INFO] 10.244.0.11:57044 - 55053 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000317833s
[INFO] 10.244.0.11:57044 - 55761 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000514042s
[INFO] 10.244.0.11:48247 - 43081 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000231166s
[INFO] 10.244.0.11:48247 - 42623 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000187375s
[INFO] 10.244.0.11:54381 - 43020 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000117875s
[INFO] 10.244.0.11:54381 - 42770 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000166042s
[INFO] 10.244.0.11:47592 - 20586 "A IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.010037625s
[INFO] 10.244.0.11:47592 - 20753 "AAAA IN mongo. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.010272042s
[INFO] 10.244.0.11:52605 - 23218 "A IN my-name-mongodb-servive.default.svc.cluster.local. udp 67 false 512" NOERROR qr,aa,rd 132 0.000194291s
[INFO] 10.244.0.11:52605 - 23385 "AAAA IN my-name-mongodb-servive.default.svc.cluster.local. udp 67 false 512" NOERROR qr,aa,rd 160 0.000262667s
[INFO] 10.244.0.11:38271 - 55741 "AAAA IN my-name-mongodb-servive.default.svc.cluster.local. udp 67 false 512" NOERROR qr,aa,rd 160 0.0001055s
[INFO] 10.244.0.11:38271 - 55318 "A IN my-name-mongodb-servive.default.svc.cluster.local. udp 67 false 512" NOERROR qr,aa,rd 132 0.000095417s
[INFO] 10.244.0.11:53798 - 17757 "AAAA IN my-name-mongodb-servive.default.svc.cluster.local. udp 67 false 512" NOERROR qr,aa,rd 160 0.001113833s
[INFO] 10.244.0.11:53798 - 17132 "A IN my-name-mongodb-servive.default.svc.cluster.local. udp 67 false 512" NOERROR qr,aa,rd 132 0.001486542s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_01_19T12_31_40_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 19 Jan 2025 18:31:37 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 20 Jan 2025 03:10:14 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 20 Jan 2025 03:08:42 +0000   Sun, 19 Jan 2025 18:31:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 20 Jan 2025 03:08:42 +0000   Sun, 19 Jan 2025 18:31:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 20 Jan 2025 03:08:42 +0000   Sun, 19 Jan 2025 18:31:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 20 Jan 2025 03:08:42 +0000   Sun, 19 Jan 2025 18:31:37 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             2036420Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             2036420Ki
  pods:               110
System Info:
  Machine ID:                 84d629821f174879aba5f0df8c99abb2
  System UUID:                84d629821f174879aba5f0df8c99abb2
  Boot ID:                    4ebdb635-858c-4052-a530-76fa5cfe66ee
  Kernel Version:             5.10.76-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     mongo-express-5dd87b9fcf-5j522               0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h52m
  default                     my-name-mongodb-deployment-5b8c785c-ck8zv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h29m
  kube-system                 coredns-668d6bf9bc-b6hwf                     100m (2%)     0 (0%)      70Mi (3%)        170Mi (8%)     8h
  kube-system                 etcd-minikube                                100m (2%)     0 (0%)      100Mi (5%)       0 (0%)         8h
  kube-system                 kube-apiserver-minikube                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 kube-controller-manager-minikube             200m (5%)     0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 kube-proxy-nj4hn                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 kube-scheduler-minikube                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (8%)  170Mi (8%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Jan19 22:22] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.019619] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +5.531321] grpcfuse: loading out-of-tree module taints kernel.
[Jan19 22:24] tmpfs: Unknown parameter 'noswap'
[  +3.969825] tmpfs: Unknown parameter 'noswap'
[Jan19 22:25] hrtimer: interrupt took 8308625 ns


==> etcd [d1b9f27afd04] <==
{"level":"info","ts":"2025-01-19T21:57:17.471818Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9944}
{"level":"info","ts":"2025-01-19T21:57:17.475059Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":9944,"took":"2.424875ms","hash":2932194275,"current-db-size-bytes":1720320,"current-db-size":"1.7 MB","current-db-size-in-use-bytes":876544,"current-db-size-in-use":"876 kB"}
{"level":"info","ts":"2025-01-19T21:57:17.475122Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2932194275,"revision":9944,"compact-revision":9704}
{"level":"info","ts":"2025-01-19T22:02:17.490933Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10183}
{"level":"info","ts":"2025-01-19T22:02:17.497403Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":10183,"took":"4.669667ms","hash":3688587433,"current-db-size-bytes":1720320,"current-db-size":"1.7 MB","current-db-size-in-use-bytes":995328,"current-db-size-in-use":"995 kB"}
{"level":"info","ts":"2025-01-19T22:02:17.497476Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3688587433,"revision":10183,"compact-revision":9944}
{"level":"info","ts":"2025-01-19T22:07:33.042058Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10427}
{"level":"info","ts":"2025-01-19T22:07:33.046040Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":10427,"took":"3.166292ms","hash":351278641,"current-db-size-bytes":1843200,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1114112,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2025-01-19T22:07:33.046075Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":351278641,"revision":10427,"compact-revision":10183}
{"level":"info","ts":"2025-01-19T22:12:33.045927Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10668}
{"level":"info","ts":"2025-01-19T22:12:33.049747Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":10668,"took":"2.393625ms","hash":339101205,"current-db-size-bytes":1843200,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":991232,"current-db-size-in-use":"991 kB"}
{"level":"info","ts":"2025-01-19T22:12:33.049798Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":339101205,"revision":10668,"compact-revision":10427}
{"level":"info","ts":"2025-01-19T22:17:33.053058Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10908}
{"level":"info","ts":"2025-01-19T22:17:33.055993Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":10908,"took":"2.224667ms","hash":1336243530,"current-db-size-bytes":1843200,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":983040,"current-db-size-in-use":"983 kB"}
{"level":"info","ts":"2025-01-19T22:17:33.056042Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1336243530,"revision":10908,"compact-revision":10668}
{"level":"info","ts":"2025-01-19T22:22:33.058085Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11148}
{"level":"info","ts":"2025-01-19T22:22:33.067969Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":11148,"took":"8.478417ms","hash":1586615104,"current-db-size-bytes":1843200,"current-db-size":"1.8 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-01-19T22:22:33.067998Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1586615104,"revision":11148,"compact-revision":10908}
{"level":"info","ts":"2025-01-19T22:27:33.027304Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11482}
{"level":"info","ts":"2025-01-19T22:27:33.032613Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":11482,"took":"4.685125ms","hash":3487208672,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1687552,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-01-19T22:27:33.032667Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3487208672,"revision":11482,"compact-revision":11148}
{"level":"info","ts":"2025-01-19T22:32:33.032334Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11761}
{"level":"info","ts":"2025-01-19T22:32:33.036605Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":11761,"took":"3.169333ms","hash":625013438,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1208320,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-01-19T22:32:33.036653Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":625013438,"revision":11761,"compact-revision":11482}
{"level":"info","ts":"2025-01-19T22:37:33.033347Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12017}
{"level":"info","ts":"2025-01-19T22:37:33.037527Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":12017,"took":"3.252167ms","hash":1130524700,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1163264,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-01-19T22:37:33.037591Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1130524700,"revision":12017,"compact-revision":11761}
{"level":"info","ts":"2025-01-19T22:42:33.034489Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12270}
{"level":"info","ts":"2025-01-19T22:42:33.036801Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":12270,"took":"1.8205ms","hash":827364311,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1146880,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2025-01-19T22:42:33.036863Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":827364311,"revision":12270,"compact-revision":12017}
{"level":"info","ts":"2025-01-19T22:47:13.991052Z","caller":"traceutil/trace.go:171","msg":"trace[1931297457] linearizableReadLoop","detail":"{readStateIndex:15847; appliedIndex:15847; }","duration":"200.262708ms","start":"2025-01-19T22:47:13.786543Z","end":"2025-01-19T22:47:13.986805Z","steps":["trace[1931297457] 'read index received'  (duration: 200.217667ms)","trace[1931297457] 'applied index is now lower than readState.Index'  (duration: 39.291µs)"],"step_count":2}
{"level":"warn","ts":"2025-01-19T22:47:13.996657Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"205.741833ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/\" range_end:\"/registry/endpointslices0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-01-19T22:47:13.996922Z","caller":"traceutil/trace.go:171","msg":"trace[1108984650] range","detail":"{range_begin:/registry/endpointslices/; range_end:/registry/endpointslices0; response_count:0; response_revision:12758; }","duration":"210.282167ms","start":"2025-01-19T22:47:13.786462Z","end":"2025-01-19T22:47:13.996744Z","steps":["trace[1108984650] 'agreement among raft nodes before linearized reading'  (duration: 205.247083ms)"],"step_count":1}
{"level":"info","ts":"2025-01-19T22:47:33.038006Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12524}
{"level":"info","ts":"2025-01-19T22:47:33.041469Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":12524,"took":"2.424833ms","hash":875338883,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1126400,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2025-01-19T22:47:33.041502Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":875338883,"revision":12524,"compact-revision":12270}
{"level":"info","ts":"2025-01-19T22:52:33.039022Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12775}
{"level":"info","ts":"2025-01-19T22:52:33.047280Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":12775,"took":"6.136958ms","hash":3975777002,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1171456,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-01-19T22:52:33.047335Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3975777002,"revision":12775,"compact-revision":12524}
{"level":"info","ts":"2025-01-19T22:57:33.042145Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13014}
{"level":"info","ts":"2025-01-19T22:57:33.045302Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":13014,"took":"2.424833ms","hash":3475169198,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1007616,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2025-01-19T22:57:33.045342Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3475169198,"revision":13014,"compact-revision":12775}
{"level":"info","ts":"2025-01-19T23:02:33.044807Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13250}
{"level":"info","ts":"2025-01-19T23:02:33.048144Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":13250,"took":"2.489334ms","hash":3148216971,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":999424,"current-db-size-in-use":"999 kB"}
{"level":"info","ts":"2025-01-19T23:02:33.048226Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3148216971,"revision":13250,"compact-revision":13014}
{"level":"info","ts":"2025-01-19T23:07:33.008229Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13488}
{"level":"info","ts":"2025-01-19T23:07:33.019692Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":13488,"took":"10.000583ms","hash":3554221375,"current-db-size-bytes":1961984,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1011712,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2025-01-19T23:07:33.019803Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3554221375,"revision":13488,"compact-revision":13250}
{"level":"info","ts":"2025-01-19T23:12:33.002982Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13729}
{"level":"info","ts":"2025-01-19T23:12:33.006290Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":13729,"took":"2.589125ms","hash":522757653,"current-db-size-bytes":2420736,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1699840,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-01-19T23:12:33.006593Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":522757653,"revision":13729,"compact-revision":13488}
{"level":"info","ts":"2025-01-19T23:14:52.241790Z","caller":"traceutil/trace.go:171","msg":"trace[315355114] linearizableReadLoop","detail":"{readStateIndex:17520; appliedIndex:17520; }","duration":"164.210792ms","start":"2025-01-19T23:14:52.076955Z","end":"2025-01-19T23:14:52.241165Z","steps":["trace[315355114] 'read index received'  (duration: 164.20675ms)","trace[315355114] 'applied index is now lower than readState.Index'  (duration: 3.333µs)"],"step_count":2}
{"level":"warn","ts":"2025-01-19T23:14:52.242999Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"165.581375ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-01-19T23:14:52.243122Z","caller":"traceutil/trace.go:171","msg":"trace[274391515] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:14093; }","duration":"166.104125ms","start":"2025-01-19T23:14:52.076943Z","end":"2025-01-19T23:14:52.243047Z","steps":["trace[274391515] 'agreement among raft nodes before linearized reading'  (duration: 165.569ms)"],"step_count":1}
{"level":"info","ts":"2025-01-19T23:17:33.006772Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13982}
{"level":"info","ts":"2025-01-19T23:17:33.010696Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":13982,"took":"3.122875ms","hash":3908337246,"current-db-size-bytes":2420736,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1703936,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-01-19T23:17:33.010739Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3908337246,"revision":13982,"compact-revision":13729}
{"level":"info","ts":"2025-01-20T03:09:05.485890Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14221}
{"level":"info","ts":"2025-01-20T03:09:05.497462Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":14221,"took":"10.597125ms","hash":1749937325,"current-db-size-bytes":2420736,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1110016,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2025-01-20T03:09:05.497625Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1749937325,"revision":14221,"compact-revision":13982}


==> kernel <==
 03:10:17 up  4:48,  0 users,  load average: 0.26, 0.34, 0.31
Linux minikube 5.10.76-linuxkit #1 SMP PREEMPT Mon Nov 8 11:22:26 UTC 2021 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [f2df69cd1763] <==
I0119 18:31:37.351674       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0119 18:31:37.351718       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0119 18:31:37.351730       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0119 18:31:37.355710       1 aggregator.go:169] waiting for initial CRD sync...
I0119 18:31:37.355761       1 local_available_controller.go:156] Starting LocalAvailability controller
I0119 18:31:37.355805       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0119 18:31:37.355989       1 controller.go:142] Starting OpenAPI controller
I0119 18:31:37.356008       1 controller.go:90] Starting OpenAPI V3 controller
I0119 18:31:37.356015       1 naming_controller.go:294] Starting NamingConditionController
I0119 18:31:37.356032       1 establishing_controller.go:81] Starting EstablishingController
I0119 18:31:37.356046       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0119 18:31:37.356058       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0119 18:31:37.356064       1 crd_finalizer.go:269] Starting CRDFinalizer
I0119 18:31:37.356241       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0119 18:31:37.356264       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0119 18:31:37.377930       1 shared_informer.go:320] Caches are synced for node_authorizer
E0119 18:31:37.432093       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0119 18:31:37.433172       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0119 18:31:37.433192       1 policy_source.go:240] refreshing policies
I0119 18:31:37.448422       1 shared_informer.go:320] Caches are synced for configmaps
I0119 18:31:37.448469       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0119 18:31:37.448492       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0119 18:31:37.448507       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0119 18:31:37.449984       1 controller.go:615] quota admission added evaluator for: namespaces
I0119 18:31:37.451663       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0119 18:31:37.451802       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0119 18:31:37.451811       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0119 18:31:37.455838       1 cache.go:39] Caches are synced for LocalAvailability controller
I0119 18:31:37.456296       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0119 18:31:37.456314       1 aggregator.go:171] initial CRD sync complete...
I0119 18:31:37.456325       1 autoregister_controller.go:144] Starting autoregister controller
I0119 18:31:37.456328       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0119 18:31:37.456331       1 cache.go:39] Caches are synced for autoregister controller
I0119 18:31:37.634339       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0119 18:31:38.357942       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0119 18:31:38.362469       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0119 18:31:38.362494       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0119 18:31:38.577151       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0119 18:31:38.594955       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0119 18:31:38.656810       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0119 18:31:38.659766       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0119 18:31:38.660311       1 controller.go:615] quota admission added evaluator for: endpoints
I0119 18:31:38.662127       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0119 18:31:39.397824       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0119 18:31:39.483301       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0119 18:31:39.488427       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0119 18:31:39.493427       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0119 18:31:44.695445       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0119 18:31:44.944658       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0119 19:42:48.889812       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-service" clusterIPs={"IPv4":"10.108.245.128"}
E0119 21:00:18.330881       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0119 21:00:18.343025       1 wrap.go:53] "Timeout or abort while handling" logger="UnhandledError" method="GET" URI="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" auditID="69e897ce-e04c-4115-b05e-5b8deed1f842"
E0119 21:00:18.415755       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="27.292µs" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
I0119 21:58:56.038069       1 alloc.go:330] "allocated clusterIPs" service="default/my-name-mongodb-servive" clusterIPs={"IPv4":"10.109.220.243"}
I0119 22:17:56.700716       1 alloc.go:330] "allocated clusterIPs" service="default/mongo-express-service" clusterIPs={"IPv4":"10.96.205.118"}
I0119 23:10:04.757148       1 alloc.go:330] "allocated clusterIPs" service="default/mongo-express" clusterIPs={"IPv4":"10.108.140.237"}
E0119 23:51:32.050085       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0119 23:51:32.050118       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0120 02:12:55.725183       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0120 02:12:55.725183       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"


==> kube-controller-manager [69a8638dafce] <==
I0119 22:18:46.682201       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="49.166µs"
I0119 22:18:47.730059       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="10.997375ms"
I0119 22:18:47.730855       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="190.791µs"
I0119 22:19:28.113411       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="8.404208ms"
I0119 22:19:28.115707       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="39.75µs"
I0119 22:19:39.466266       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="216.041µs"
I0119 22:19:40.211416       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="7.095167ms"
I0119 22:19:40.211551       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="97.292µs"
I0119 22:20:20.730529       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="14.31625ms"
I0119 22:20:20.731935       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="125.208µs"
I0119 22:20:35.454901       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="335.875µs"
I0119 22:20:48.080600       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="15.291917ms"
I0119 22:20:48.081658       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="56.583µs"
I0119 22:20:54.417761       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 22:21:27.472209       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="14.974666ms"
I0119 22:21:27.472402       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="89.458µs"
I0119 22:21:39.470398       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="905.875µs"
I0119 22:22:15.055945       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="11.429875ms"
I0119 22:22:15.057815       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="42.958µs"
I0119 22:22:54.374863       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="10.191833ms"
I0119 22:22:54.376547       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="16.667µs"
I0119 22:23:08.484281       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="267.083µs"
I0119 22:24:22.221077       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="14.236417ms"
I0119 22:24:22.221169       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="51.916µs"
I0119 22:25:03.335445       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="50.993042ms"
I0119 22:25:03.338222       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="58.208µs"
I0119 22:25:17.419606       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="2.459083ms"
I0119 22:26:00.469968       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 22:27:54.500765       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="8.515667ms"
I0119 22:27:54.500881       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="46.708µs"
I0119 22:28:34.863487       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="7.519333ms"
I0119 22:28:34.863720       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="170.333µs"
I0119 22:28:49.411726       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="62.25µs"
I0119 22:31:07.234055       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 22:33:36.559074       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="20.632376ms"
I0119 22:33:36.559546       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="26.625µs"
I0119 22:34:16.014036       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="9.646625ms"
I0119 22:34:16.014181       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="54.125µs"
I0119 22:34:29.406618       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="267.875µs"
I0119 22:36:12.412719       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 22:39:26.225543       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="12.719625ms"
I0119 22:39:26.228580       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="21.375µs"
I0119 22:40:05.652534       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="8.332667ms"
I0119 22:40:05.653417       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="60.458µs"
I0119 22:40:17.400697       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="73.75µs"
I0119 22:41:18.463628       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 22:45:12.022209       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="12.319208ms"
I0119 22:45:12.022284       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-5dd87b9fcf" duration="42.958µs"
I0119 22:46:25.798054       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 22:51:31.717501       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 22:56:37.952430       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 23:01:44.105761       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 23:06:51.151153       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 23:11:57.280453       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0119 23:17:04.171502       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0119 23:51:32.058959       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0119 23:51:32.075902       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0120 02:12:55.726291       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0120 02:12:55.726364       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0120 03:08:42.905042       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [e7664f2707e6] <==
I0119 18:31:45.313665       1 server_linux.go:66] "Using iptables proxy"
I0119 18:31:45.380291       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0119 18:31:45.380344       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0119 18:31:45.393876       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0119 18:31:45.393914       1 server_linux.go:170] "Using iptables Proxier"
I0119 18:31:45.395555       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0119 18:31:45.396499       1 server.go:497] "Version info" version="v1.32.0"
I0119 18:31:45.396511       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0119 18:31:45.398701       1 config.go:199] "Starting service config controller"
I0119 18:31:45.398959       1 shared_informer.go:313] Waiting for caches to sync for service config
I0119 18:31:45.399008       1 config.go:329] "Starting node config controller"
I0119 18:31:45.399011       1 shared_informer.go:313] Waiting for caches to sync for node config
I0119 18:31:45.399020       1 config.go:105] "Starting endpoint slice config controller"
I0119 18:31:45.399022       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0119 18:31:45.499401       1 shared_informer.go:320] Caches are synced for service config
I0119 18:31:45.499401       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0119 18:31:45.499412       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [1c515e97750d] <==
I0119 18:31:36.429684       1 serving.go:386] Generated self-signed cert in-memory
W0119 18:31:37.387238       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0119 18:31:37.387271       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0119 18:31:37.387279       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0119 18:31:37.387284       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0119 18:31:37.397117       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0119 18:31:37.397141       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0119 18:31:37.398841       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0119 18:31:37.398915       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0119 18:31:37.399023       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0119 18:31:37.399277       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0119 18:31:37.401671       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0119 18:31:37.401786       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0119 18:31:37.401821       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0119 18:31:37.401893       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.401705       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0119 18:31:37.402075       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.401725       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0119 18:31:37.402097       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.401751       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0119 18:31:37.402105       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.401768       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0119 18:31:37.402111       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.402019       1 reflector.go:569] runtime/asm_arm64.s:1223: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0119 18:31:37.402144       1 reflector.go:166] "Unhandled Error" err="runtime/asm_arm64.s:1223: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0119 18:31:37.402320       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0119 18:31:37.402333       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.403304       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0119 18:31:37.403328       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0119 18:31:37.403370       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0119 18:31:37.403369       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0119 18:31:37.403377       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.403358       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0119 18:31:37.403387       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0119 18:31:37.403329       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.403414       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0119 18:31:37.403427       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.403448       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0119 18:31:37.403461       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.403467       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0119 18:31:37.403475       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0119 18:31:37.405202       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0119 18:31:37.405222       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0119 18:31:38.221935       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0119 18:31:38.222004       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0119 18:31:38.326812       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0119 18:31:38.326912       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0119 18:31:38.380379       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0119 18:31:38.380424       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0119 18:31:38.434472       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0119 18:31:38.434695       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0119 18:31:38.460617       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0119 18:31:38.460644       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0119 18:31:38.899701       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jan 19 22:38:14 minikube kubelet[2311]: E0119 22:38:14.381200    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:38:28 minikube kubelet[2311]: I0119 22:38:28.378958    2311 scope.go:117] "RemoveContainer" containerID="b3b5e6c38c8237178faea09e191dbebdc8c0e483e4383b57d96846183bbc61cf"
Jan 19 22:38:28 minikube kubelet[2311]: E0119 22:38:28.379619    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:38:42 minikube kubelet[2311]: I0119 22:38:42.378791    2311 scope.go:117] "RemoveContainer" containerID="b3b5e6c38c8237178faea09e191dbebdc8c0e483e4383b57d96846183bbc61cf"
Jan 19 22:38:42 minikube kubelet[2311]: E0119 22:38:42.380036    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:38:57 minikube kubelet[2311]: I0119 22:38:57.378696    2311 scope.go:117] "RemoveContainer" containerID="b3b5e6c38c8237178faea09e191dbebdc8c0e483e4383b57d96846183bbc61cf"
Jan 19 22:38:57 minikube kubelet[2311]: E0119 22:38:57.379147    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:39:12 minikube kubelet[2311]: I0119 22:39:12.385324    2311 scope.go:117] "RemoveContainer" containerID="b3b5e6c38c8237178faea09e191dbebdc8c0e483e4383b57d96846183bbc61cf"
Jan 19 22:39:12 minikube kubelet[2311]: E0119 22:39:12.385567    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:39:24 minikube kubelet[2311]: I0119 22:39:24.380183    2311 scope.go:117] "RemoveContainer" containerID="b3b5e6c38c8237178faea09e191dbebdc8c0e483e4383b57d96846183bbc61cf"
Jan 19 22:40:05 minikube kubelet[2311]: I0119 22:40:05.627265    2311 scope.go:117] "RemoveContainer" containerID="b3b5e6c38c8237178faea09e191dbebdc8c0e483e4383b57d96846183bbc61cf"
Jan 19 22:40:05 minikube kubelet[2311]: I0119 22:40:05.627527    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:40:05 minikube kubelet[2311]: E0119 22:40:05.627839    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:40:17 minikube kubelet[2311]: I0119 22:40:17.378595    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:40:17 minikube kubelet[2311]: E0119 22:40:17.379094    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:40:30 minikube kubelet[2311]: I0119 22:40:30.392318    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:40:30 minikube kubelet[2311]: E0119 22:40:30.395323    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:40:42 minikube kubelet[2311]: I0119 22:40:42.376480    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:40:42 minikube kubelet[2311]: E0119 22:40:42.377077    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:40:54 minikube kubelet[2311]: I0119 22:40:54.376364    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:40:54 minikube kubelet[2311]: E0119 22:40:54.376767    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:41:06 minikube kubelet[2311]: I0119 22:41:06.376800    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:41:06 minikube kubelet[2311]: E0119 22:41:06.376956    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:41:20 minikube kubelet[2311]: I0119 22:41:20.378315    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:41:20 minikube kubelet[2311]: E0119 22:41:20.380318    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:41:31 minikube kubelet[2311]: I0119 22:41:31.377382    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:41:31 minikube kubelet[2311]: E0119 22:41:31.378043    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:41:45 minikube kubelet[2311]: I0119 22:41:45.380392    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:41:45 minikube kubelet[2311]: E0119 22:41:45.382004    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:41:59 minikube kubelet[2311]: I0119 22:41:59.380740    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:41:59 minikube kubelet[2311]: E0119 22:41:59.382378    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:42:11 minikube kubelet[2311]: I0119 22:42:11.377913    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:42:11 minikube kubelet[2311]: E0119 22:42:11.378337    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:42:22 minikube kubelet[2311]: I0119 22:42:22.378173    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:42:22 minikube kubelet[2311]: E0119 22:42:22.381058    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:42:34 minikube kubelet[2311]: I0119 22:42:34.376801    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:42:34 minikube kubelet[2311]: E0119 22:42:34.377195    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:42:46 minikube kubelet[2311]: I0119 22:42:46.377242    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:42:46 minikube kubelet[2311]: E0119 22:42:46.378113    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:42:59 minikube kubelet[2311]: I0119 22:42:59.376759    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:42:59 minikube kubelet[2311]: E0119 22:42:59.377798    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:43:12 minikube kubelet[2311]: I0119 22:43:12.378040    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:43:12 minikube kubelet[2311]: E0119 22:43:12.378808    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:43:23 minikube kubelet[2311]: I0119 22:43:23.376812    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:43:23 minikube kubelet[2311]: E0119 22:43:23.377843    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:43:38 minikube kubelet[2311]: I0119 22:43:38.377670    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:43:38 minikube kubelet[2311]: E0119 22:43:38.378223    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:43:50 minikube kubelet[2311]: I0119 22:43:50.377611    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:43:50 minikube kubelet[2311]: E0119 22:43:50.378846    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:44:05 minikube kubelet[2311]: I0119 22:44:05.377007    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:44:05 minikube kubelet[2311]: E0119 22:44:05.383709    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:44:16 minikube kubelet[2311]: I0119 22:44:16.375259    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:44:16 minikube kubelet[2311]: E0119 22:44:16.375547    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:44:30 minikube kubelet[2311]: I0119 22:44:30.377418    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:44:30 minikube kubelet[2311]: E0119 22:44:30.380748    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:44:43 minikube kubelet[2311]: I0119 22:44:43.377832    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:44:43 minikube kubelet[2311]: E0119 22:44:43.378719    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:44:58 minikube kubelet[2311]: I0119 22:44:58.375546    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"
Jan 19 22:44:58 minikube kubelet[2311]: E0119 22:44:58.375807    2311 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-5dd87b9fcf-5j522_default(d61324b8-95e3-45b3-9c4f-ef5e40ce81f1)\"" pod="default/mongo-express-5dd87b9fcf-5j522" podUID="d61324b8-95e3-45b3-9c4f-ef5e40ce81f1"
Jan 19 22:45:10 minikube kubelet[2311]: I0119 22:45:10.375702    2311 scope.go:117] "RemoveContainer" containerID="0cf4eecf1d065177a44983e8db2280dd28d7a68f2ec7ac40dff5050f2402dd2f"


==> storage-provisioner [52a8876e2266] <==
encoding/json.(*Decoder).Decode(0x40003f8c60, 0xefe300, 0x4000630288, 0x0, 0x0)
	/usr/local/go/src/encoding/json/stream.go:63 +0x60
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0x40001f74d0, 0x4000498000, 0x400, 0x400, 0x4000461d00, 0x15400, 0x4000461d00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x194
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0x4000605f40, 0x0, 0x126f7a8, 0x40006032c0, 0x0, 0x0, 0x6cad0, 0x400007dc20, 0x4000481648)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x70
k8s.io/client-go/rest/watch.(*Decoder).Decode(0x400040b680, 0x4000461ef0, 0x8, 0x126f1e0, 0x4000251380, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x60
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0x40006033c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0xe8
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xb4

goroutine 12771 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0x400064d900, 0x4000000006)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0x400064d8f0)
	/usr/local/go/src/sync/cond.go:56 +0xb8
golang.org/x/net/http2.(*pipe).Read(0x400064d8e8, 0x400069a601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x94
golang.org/x/net/http2.transportResponseBody.Read(0x400064d8c0, 0x400069a601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0x80
encoding/json.(*Decoder).refill(0x400064db80, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xd4
encoding/json.(*Decoder).readValue(0x400064db80, 0x0, 0x0, 0x7952ac)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1b8
encoding/json.(*Decoder).Decode(0x400064db80, 0xefe300, 0x40001839b0, 0x0, 0x0)
	/usr/local/go/src/encoding/json/stream.go:63 +0x60
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0x40001fccc0, 0x40005d6800, 0x400, 0x400, 0x400045fd00, 0x15400, 0x400045fd00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x194
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0x400037a5f0, 0x0, 0x126f7a8, 0x4000602dc0, 0x0, 0x0, 0x6cad0, 0x400046e6c0, 0x4000481e48)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x70
k8s.io/client-go/rest/watch.(*Decoder).Decode(0x4000256400, 0x400045fef0, 0x8, 0x126e088, 0x400033e000, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x60
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0x4000014500)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0xe8
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xb4

goroutine 13285 [runnable]:
k8s.io/client-go/tools/record.(*recorderImpl).generateEvent.func1(0x40000db880, 0x40000e2000)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/tools/record/event.go:341
created by k8s.io/client-go/tools/record.(*recorderImpl).generateEvent
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/tools/record/event.go:341 +0x31c

goroutine 12835 [runnable]:
encoding/json.Unmarshal(0x40001b1400, 0x9f, 0x400, 0xe5f880, 0x4000256800, 0x4000136240, 0x40006819d8)
	/usr/local/go/src/encoding/json/decode.go:96 +0x164
k8s.io/apimachinery/pkg/runtime/serializer/json.SimpleMetaFactory.Interpret(0x40001b1400, 0x9f, 0x400, 0x4000500b20, 0xa0, 0x8a1f0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/json/meta.go:55 +0x60
k8s.io/apimachinery/pkg/runtime/serializer/json.(*Serializer).Decode(0x400007aaa0, 0x40001b1400, 0x9f, 0x400, 0x0, 0x126f7a8, 0x40006026c0, 0x6f530, 0xffff643152d0, 0x38, ...)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/json/json.go:219 +0x68
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0x400037b630, 0x0, 0x126f7a8, 0x40006026c0, 0x0, 0x0, 0x6cad0, 0x400046f980, 0x4000274e48)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:107 +0x114
k8s.io/client-go/rest/watch.(*Decoder).Decode(0x4000256940, 0x4000681ef0, 0x8, 0x126e060, 0x4000128a00, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x60
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0x4000015000)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0xe8
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xb4


==> storage-provisioner [656d3fc6120c] <==
I0119 21:00:19.504555       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0119 21:00:19.524344       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0119 21:00:19.524477       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0119 21:00:35.056078       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0119 21:00:35.056666       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_12f61a57-6ef6-4d4a-a1d1-1a6ebc92073e!
I0119 21:00:35.056485       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"cc81f8cb-eebd-4460-8e0a-183981a9dfaa", APIVersion:"v1", ResourceVersion:"7699", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_12f61a57-6ef6-4d4a-a1d1-1a6ebc92073e became leader
I0119 21:00:35.169222       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_12f61a57-6ef6-4d4a-a1d1-1a6ebc92073e!

